{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyZTa6Ie93QA"
      },
      "source": [
        "# Fluxentropy\n",
        "By [Green](https://x.com/myainotez) and [Blue](https://x.com/tensorqt) knights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkZ6aNBp-6Gb"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hahhj8LPlhgG"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers.utils'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     logging,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.utils'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from google.colab import userdata\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login(userdata.get('HF_TOKEN'))\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    torch.cuda.empty_cache()\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "class EntropixModel:\n",
        "    def __init__(self, model, tokenizer, seed: int = 1337, dtype: torch.dtype = torch.bfloat16):\n",
        "        self.model = model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dtype = dtype\n",
        "        self.seed = seed\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    def entropy_characterize(\n",
        "        self,\n",
        "        input_strings: list,\n",
        "        config: dict,\n",
        "        max_length: int = 512\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Computes specified characteristics based on the configuration for a batch of input strings.\n",
        "\n",
        "        Args:\n",
        "            input_strings (list): List of input strings to analyze.\n",
        "            config (dict): Configuration dictionary specifying characteristics and mechanism.\n",
        "                Example:\n",
        "                {\n",
        "                    \"mechanism\": \"per_token\",  # or \"per_string\"\n",
        "                    \"compute_entropy\": True,\n",
        "                    \"compute_varentropy\": False,\n",
        "                    \"compute_additional_metric\": True,\n",
        "                    ...\n",
        "                }\n",
        "            max_length (int): Maximum sequence length for tokenization.\n",
        "\n",
        "        Returns:\n",
        "            dict or tensor/list: Depending on the configuration, returns a dictionary of characteristics\n",
        "                                 or a list/tensor with characteristics per token.\n",
        "        \"\"\"\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        # Tokenize input strings with padding and truncation\n",
        "        encodings = self.tokenizer(\n",
        "            input_strings,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "\n",
        "        input_ids = encodings[\"input_ids\"].to(device)\n",
        "        padding_mask = encodings[\"attention_mask\"].to(device)\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        results = {\n",
        "            \"input_strings\": input_strings,\n",
        "            \"tokens\": [self.tokenizer.convert_ids_to_tokens(ids) for ids in input_ids],\n",
        "            \"attention_mask\": padding_mask.cpu()\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=padding_mask)\n",
        "            logits = outputs.logits  # Shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "            mechanism = config.get(\"mechanism\", \"per_token\")\n",
        "            compute_entropy = config.get(\"compute_entropy\", False)\n",
        "            compute_varentropy = config.get(\"compute_varentropy\", False)\n",
        "            # Add more characteristics as needed\n",
        "\n",
        "            if mechanism == \"per_token\":\n",
        "                if compute_entropy or compute_varentropy:\n",
        "                    entropy_list = []\n",
        "                    varentropy_list = []\n",
        "\n",
        "                    for i in range(seq_len):\n",
        "                        logits_i = logits[:, i, :]  # Logits for position i\n",
        "                        log_probs = torch.log_softmax(logits_i, dim=-1)\n",
        "                        probs = torch.exp(log_probs)\n",
        "\n",
        "                        if compute_entropy:\n",
        "                            entropy = -torch.sum(probs * log_probs, dim=-1)  # Shape: (batch_size,)\n",
        "                            entropy_list.append(entropy)\n",
        "                        if compute_varentropy:\n",
        "                            if compute_entropy:\n",
        "                                entropy_unsqueezed = entropy.unsqueeze(-1)\n",
        "                            else:\n",
        "                                entropy_unsqueezed = torch.sum(probs * log_probs, dim=-1).unsqueeze(-1)\n",
        "                            varentropy = torch.sum(probs * (log_probs + entropy_unsqueezed) ** 2, dim=-1)\n",
        "                            varentropy_list.append(varentropy)\n",
        "\n",
        "                    if compute_entropy:\n",
        "                        entropy_tensor = torch.stack(entropy_list, dim=1)  # Shape: (batch_size, seq_len)\n",
        "                        results[\"entropy\"] = (entropy_tensor * padding_mask).cpu()\n",
        "\n",
        "                    if compute_varentropy:\n",
        "                        varentropy_tensor = torch.stack(varentropy_list, dim=1)  # Shape: (batch_size, seq_len)\n",
        "                        results[\"varentropy\"] = (varentropy_tensor * padding_mask).cpu()\n",
        "\n",
        "                # Add additional characteristics computations here\n",
        "\n",
        "            elif mechanism == \"per_string\":\n",
        "                if compute_entropy or compute_varentropy:\n",
        "                    # Get logits for the next token after the full input\n",
        "                    next_token_logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
        "                    log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
        "                    probs = torch.exp(log_probs)\n",
        "\n",
        "                    if compute_entropy:\n",
        "                        entropy = -torch.sum(probs * log_probs, dim=-1)  # Shape: (batch_size,)\n",
        "                        results[\"entropy\"] = entropy.cpu()\n",
        "\n",
        "                    if compute_varentropy:\n",
        "                        varentropy = torch.sum(probs * (log_probs + entropy.unsqueeze(-1)) ** 2, dim=-1)\n",
        "                        results[\"varentropy\"] = varentropy.cpu()\n",
        "\n",
        "                # Add additional characteristics computations here\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown mechanism: {mechanism}\")\n",
        "\n",
        "        # Depending on config, decide the output format\n",
        "        output_format = config.get(\"output_format\", \"dict\")  # or \"tensor\"\n",
        "\n",
        "        if output_format == \"dict\":\n",
        "            return results\n",
        "        elif output_format in [\"tensor\", \"list\"]:\n",
        "            # Collect characteristics into a single tensor or list\n",
        "            characteristics = []\n",
        "            if compute_entropy:\n",
        "                characteristics.append(results[\"entropy\"])\n",
        "            if compute_varentropy:\n",
        "                characteristics.append(results[\"varentropy\"])\n",
        "            # Add additional characteristics here\n",
        "\n",
        "            if not characteristics:\n",
        "                raise ValueError(\"No characteristics were computed based on the configuration.\")\n",
        "\n",
        "            # Stack characteristics along a new dimension\n",
        "            combined = torch.stack(characteristics, dim=-1)  # Shape: (batch_size, features)\n",
        "            if output_format == \"tensor\":\n",
        "                return combined\n",
        "            else:\n",
        "                return combined.cpu().numpy().tolist()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown output_format: {output_format}\")\n",
        "\n",
        "    def visualize_results(self, results, config: dict, title=None, height=800):\n",
        "        \"\"\"\n",
        "        Creates interactive visualizations for entropy and varentropy results using Plotly.\n",
        "\n",
        "        Args:\n",
        "            results (dict or tensor/list): Output from entropy_characterize.\n",
        "            config (dict): Configuration dictionary specifying what was computed.\n",
        "            title (str, optional): Title for the visualization.\n",
        "            height (int, optional): Height of the plot in pixels.\n",
        "\n",
        "        Returns:\n",
        "            plotly.graph_objects.Figure: Interactive figure with visualizations.\n",
        "        \"\"\"\n",
        "        mechanism = config.get(\"mechanism\", \"per_token\")\n",
        "        compute_entropy = config.get(\"compute_entropy\", False)\n",
        "        compute_varentropy = config.get(\"compute_varentropy\", False)\n",
        "        # Add more characteristics as needed\n",
        "\n",
        "        if mechanism == \"per_string\":\n",
        "            # Visualization for full-string characteristics\n",
        "            fig = go.Figure()\n",
        "\n",
        "            if compute_entropy:\n",
        "                fig.add_trace(\n",
        "                    go.Bar(\n",
        "                        x=results['input_strings'],\n",
        "                        y=results['entropy'],\n",
        "                        text=results['entropy'],\n",
        "                        textposition='auto',\n",
        "                        name='Entropy'\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            if compute_varentropy:\n",
        "                fig.add_trace(\n",
        "                    go.Bar(\n",
        "                        x=results['input_strings'],\n",
        "                        y=results['varentropy'],\n",
        "                        text=results['varentropy'],\n",
        "                        textposition='auto',\n",
        "                        name='Varentropy'\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=title or 'Entropy and Varentropy Analysis (Full String)',\n",
        "                xaxis_title='Input Strings',\n",
        "                yaxis_title='Value',\n",
        "                barmode='group',\n",
        "                height=height\n",
        "            )\n",
        "\n",
        "        elif mechanism == \"per_token\":\n",
        "            # Visualization for per-token characteristics\n",
        "            fig = make_subplots(\n",
        "                rows=3, cols=1,\n",
        "                subplot_titles=('Entropy Over Tokens', 'Varentropy Over Tokens', 'Token-wise Analysis'),\n",
        "                vertical_spacing=0.1,\n",
        "                row_heights=[0.35, 0.35, 0.3]\n",
        "            )\n",
        "\n",
        "            for batch_idx, input_string in enumerate(results['input_strings']):\n",
        "                mask = results['attention_mask'][batch_idx]\n",
        "                seq_len = mask.sum().item()\n",
        "\n",
        "                tokens = results['tokens'][batch_idx][:seq_len]\n",
        "                positions = np.arange(seq_len)\n",
        "\n",
        "                if compute_entropy:\n",
        "                    entropy_values = results['entropy'][batch_idx][:seq_len].numpy()\n",
        "                    fig.add_trace(\n",
        "                        go.Scatter(\n",
        "                            x=positions,\n",
        "                            y=entropy_values,\n",
        "                            mode='lines+markers',\n",
        "                            name=f'Entropy (String {batch_idx + 1})',\n",
        "                            hovertemplate='Position: %{x}<br>Entropy: %{y:.3f}<extra></extra>'\n",
        "                        ),\n",
        "                        row=1, col=1\n",
        "                    )\n",
        "\n",
        "                if compute_varentropy:\n",
        "                    varentropy_values = results['varentropy'][batch_idx][:seq_len].numpy()\n",
        "                    fig.add_trace(\n",
        "                        go.Scatter(\n",
        "                            x=positions,\n",
        "                            y=varentropy_values,\n",
        "                            mode='lines+markers',\n",
        "                            name=f'Varentropy (String {batch_idx + 1})',\n",
        "                            hovertemplate='Position: %{x}<br>Varentropy: %{y:.3f}<extra></extra>'\n",
        "                        ),\n",
        "                        row=2, col=1\n",
        "                    )\n",
        "\n",
        "                # Token-wise heatmap for entropy and varentropy\n",
        "                heatmap_z = []\n",
        "                heatmap_y = []\n",
        "                if compute_entropy:\n",
        "                    heatmap_z.append(results[\"entropy\"][batch_idx][:seq_len].numpy())\n",
        "                    heatmap_y.append('Entropy')\n",
        "                if compute_varentropy:\n",
        "                    heatmap_z.append(results[\"varentropy\"][batch_idx][:seq_len].numpy())\n",
        "                    heatmap_y.append('Varentropy')\n",
        "\n",
        "                if heatmap_z:\n",
        "                    fig.add_trace(\n",
        "                        go.Heatmap(\n",
        "                            z=heatmap_z,\n",
        "                            x=tokens,\n",
        "                            y=heatmap_y,\n",
        "                            colorscale='Viridis',\n",
        "                            showscale=True,\n",
        "                            hoverongaps=False,\n",
        "                            hovertemplate='Token: %{x}<br>Metric: %{y}<br>Value: %{z:.3f}<extra></extra>'\n",
        "                        ),\n",
        "                        row=3, col=1\n",
        "                    )\n",
        "\n",
        "            fig.update_layout(\n",
        "                height=height,\n",
        "                showlegend=True,\n",
        "                title=title or 'Entropy and Varentropy Analysis',\n",
        "                hovermode='closest'\n",
        "            )\n",
        "\n",
        "            # Update axes labels\n",
        "            fig.update_xaxes(title_text='Token Position', row=1, col=1)\n",
        "            fig.update_xaxes(title_text='Token Position', row=2, col=1)\n",
        "            fig.update_xaxes(title_text='Tokens', row=3, col=1)\n",
        "\n",
        "            if compute_entropy:\n",
        "                fig.update_yaxes(title_text='Entropy', row=1, col=1)\n",
        "            if compute_varentropy:\n",
        "                fig.update_yaxes(title_text='Varentropy', row=2, col=1)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mechanism: {mechanism}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def permute_dataset(\n",
        "        self,\n",
        "        dataset: list,\n",
        "        config: dict,\n",
        "        sort_by: str,\n",
        "        descending: bool = False,\n",
        "        max_length: int = 512\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Permutes (sorts) the dataset based on a specified characteristic.\n",
        "\n",
        "        Args:\n",
        "            dataset (list): List of input strings.\n",
        "            config (dict): Configuration dictionary for entropy_characterize.\n",
        "            sort_by (str): The characteristic to sort by (e.g., 'entropy', 'varentropy').\n",
        "                            For per_token mechanism, specify 'entropy_token_avg', etc.\n",
        "            descending (bool): Whether to sort in descending order.\n",
        "            max_length (int): Maximum sequence length for tokenization.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (permuted_dataset, sorted_characteristics)\n",
        "        \"\"\"\n",
        "        # Compute characteristics\n",
        "        results = self.entropy_characterize(\n",
        "            input_strings=dataset,\n",
        "            config=config,\n",
        "            max_length=max_length\n",
        "        )\n",
        "\n",
        "        mechanism = config.get(\"mechanism\", \"per_token\")\n",
        "        compute_entropy = config.get(\"compute_entropy\", False)\n",
        "        compute_varentropy = config.get(\"compute_varentropy\", False)\n",
        "        # Add more characteristics as needed\n",
        "\n",
        "        if sort_by not in results:\n",
        "            # Handle per-token mechanism by aggregating per-token characteristics\n",
        "            if mechanism == \"per_token\":\n",
        "                padding_mask = results[\"attention_mask\"]\n",
        "                if sort_by == \"entropy_token_avg\" and compute_entropy:\n",
        "                    # Masked mean\n",
        "                    entropy = results[\"entropy\"]\n",
        "                    masked_entropy = torch.sum(entropy * padding_mask, dim=1) / torch.sum(padding_mask, dim=1)\n",
        "                    characteristic = masked_entropy\n",
        "                elif sort_by == \"entropy_token_sum\" and compute_entropy:\n",
        "                    entropy = results[\"entropy\"]\n",
        "                    masked_entropy = torch.sum(entropy * padding_mask, dim=1)\n",
        "                    characteristic = masked_entropy\n",
        "                elif sort_by == \"varentropy_token_avg\" and compute_varentropy:\n",
        "                    varentropy = results[\"varentropy\"]\n",
        "                    masked_varentropy = torch.sum(varentropy * padding_mask, dim=1) / torch.sum(padding_mask, dim=1)\n",
        "                    characteristic = masked_varentropy\n",
        "                elif sort_by == \"varentropy_token_sum\" and compute_varentropy:\n",
        "                    varentropy = results[\"varentropy\"]\n",
        "                    masked_varentropy = torch.sum(varentropy * padding_mask, dim=1)\n",
        "                    characteristic = masked_varentropy\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown sort_by option: {sort_by}\")\n",
        "            else:\n",
        "                raise ValueError(f\"sort_by '{sort_by}' not found in results and mechanism is '{mechanism}'\")\n",
        "        else:\n",
        "            # Per-string mechanism\n",
        "            characteristic = results[sort_by]\n",
        "\n",
        "        # Convert characteristic to numpy for sorting\n",
        "        characteristic_np = characteristic.cpu().numpy()\n",
        "\n",
        "        # Get sorted indices\n",
        "        sorted_indices = np.argsort(characteristic_np)\n",
        "        if descending:\n",
        "            sorted_indices = sorted_indices[::-1]\n",
        "\n",
        "        # Permute dataset\n",
        "        permuted_dataset = [dataset[idx] for idx in sorted_indices]\n",
        "\n",
        "        return permuted_dataset, characteristic_np[sorted_indices]\n",
        "\n",
        "    def display_sorted_characteristics(\n",
        "        self,\n",
        "        dataset: list,\n",
        "        config: dict,\n",
        "        sort_by: str,\n",
        "        descending: bool = False,\n",
        "        max_length: int = 512\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Permutes the dataset and displays the sorted characteristics.\n",
        "\n",
        "        Args:\n",
        "            dataset (list): List of input strings.\n",
        "            config (dict): Configuration dictionary for entropy_characterize.\n",
        "            sort_by (str): The characteristic to sort by.\n",
        "            descending (bool): Whether to sort in descending order.\n",
        "            max_length (int): Maximum sequence length for tokenization.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (permuted_dataset, sorted_characteristics)\n",
        "        \"\"\"\n",
        "        permuted_dataset, sorted_characteristics = self.permute_dataset(\n",
        "            dataset=dataset,\n",
        "            config=config,\n",
        "            sort_by=sort_by,\n",
        "            descending=descending,\n",
        "            max_length=max_length\n",
        "        )\n",
        "        print(f\"Dataset sorted by '{sort_by}' in {'descending' if descending else 'ascending'} order.\")\n",
        "        for idx, (string, characteristic) in enumerate(zip(permuted_dataset, sorted_characteristics)):\n",
        "            print(f\"{idx + 1}: {characteristic:.4f} - {string}\")\n",
        "        return permuted_dataset, sorted_characteristics\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    seed = 1337\n",
        "    torch.manual_seed(seed=seed)\n",
        "    #model_id = 'HuggingFaceTB/SmolLM-360M-Instruct' #No need for tokens as Smollm is not gated!\n",
        "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    entropix_model = EntropixModel(model, tokenizer, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCO90pDG-1DE"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2J8C6hd-lkr"
      },
      "outputs": [],
      "source": [
        "# Example inputs\n",
        "input_strings = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",  # Classic pangram\n",
        "    \"In quantum mechanics, particles can exist in multiple states simultaneously.\",  # Scientific\n",
        "    \"Â•πÁ´ôÂú®Á™óÂâçÔºåÊúõÁùÄËøúÊñπÁöÑÂ±±Â≥∞„ÄÇ\",  # Chinese (Looking at distant mountains)\n",
        "    \"To be, or not to be, that is the question.\",  # Literary/Shakespeare\n",
        "    \"The cryptocurrency market experienced significant volatility today.\",  # Financial news\n",
        "    \"Je pense, donc je suis.\",  # French philosophy (Descartes)\n",
        "    \"üåü Dancing under the moonlight, spirits high and hearts light. üåô\",  # Emojis and poetic\n",
        "    \"SELECT * FROM users WHERE age > 18;\",  # SQL code\n",
        "    \"The neural network achieved 98.5% accuracy on the test dataset.\",  # AI/ML\n",
        "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",  # Latin placeholder\n",
        "    \"Breaking: Major breakthrough in fusion energy announced today!\",  # News headline\n",
        "    \"def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)\",  # Python code\n",
        "    \"Step 1: Preheat oven to 350¬∞F. Step 2: Mix ingredients thoroughly.\",  # Recipe instructions\n",
        "    \"Once upon a time, in a galaxy far, far away...\",  # Story opening\n",
        "    \"Error 404: Page not found. Please check the URL and try again.\",  # Technical error\n",
        "    \"Climate change threatens biodiversity in coral reef ecosystems.\",  # Environmental\n",
        "    \"„Åä„ÅØ„Çà„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÅ‰ªäÊó•„ÅØ„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠„ÄÇ\",  # Japanese (Good morning, nice weather today)\n",
        "    \"1234567890 !@#$%^&*()_+ <>?:\\\"{}|\",  # Numbers and special characters\n",
        "    \"URGENT: Meeting rescheduled to 3PM EST - All hands required\",  # Business communication\n",
        "    \"The composition of Bach's fugues demonstrates mathematical precision.\",  # Music analysis\n",
        "    \"Das Leben ist wie ein Fahrrad. Man muss sich vorw√§rts bewegen.\",  # German (Einstein quote)\n",
        "    \"for i in range(len(array)): if array[i] > max_val: max_val = array[i]\",  # More Python code\n",
        "    \"CREATE TABLE employees (id INT PRIMARY KEY, name VARCHAR(255));\",  # SQL DDL\n",
        "    \"La vita √® bella quando si vive con passione.\",  # Italian (Life is beautiful...)\n",
        "    \"RT @SpaceX: Successful launch of Starship prototype #42! üöÄ\",  # Social media\n",
        "    \"–í —Ç–∏—Ö–æ–º –æ–º—É—Ç–µ —á–µ—Ä—Ç–∏ –≤–æ–¥—è—Ç—Å—è.\",  # Russian proverb\n",
        "    \"async function fetchData() { const response = await fetch(url); }\",  # JavaScript async\n",
        "    \"üéÆ Level Up! You've earned 1000 XP and unlocked new achievements! üèÜ\",  # Gaming with emojis\n",
        "    \"<!DOCTYPE html><html><head><title>Hello World</title></head></html>\",  # HTML\n",
        "    \"Hola mundo, ¬øc√≥mo est√°s hoy?\",  # Spanish greeting\n",
        "    \"import numpy as np; X = np.array([[1, 2], [3, 4]])\",  # Scientific Python\n",
        "    \"Breaking News: Artificial Intelligence Achieves New Milestone in Protein Folding\",  # Science news\n",
        "    \"public class HelloWorld { public static void main(String[] args) {} }\",  # Java\n",
        "    \"The mitochondria is the powerhouse of the cell.\",  # Biology\n",
        "    \"git commit -m \\\"Fix: resolve memory leak in main loop\\\"\",  # Git command\n",
        "    \"‡§Ö‡§§‡§ø‡§•‡§ø ‡§¶‡•á‡§µ‡•ã ‡§≠‡§µ:\",  # Sanskrit (Guest is God)\n",
        "    \"try { throw new Error('Test'); } catch (e) { console.log(e); }\",  # JavaScript error handling\n",
        "    \"Dans les champs de l'observation, le hasard ne favorise que les esprits pr√©par√©s.\",  # French (Pasteur)\n",
        "    \"docker run -d -p 80:80 nginx:latest\",  # Docker command\n",
        "    \"While(true) { System.out.println(\\\"Hello, World!\\\"); }\",  # Infinite loop\n",
        "    \"kubectl get pods -n kubernetes-dashboard\",  # Kubernetes command\n",
        "    \"ŒßŒ±ŒØœÅŒµœÑŒµ! Œ†œéœÇ ŒµŒØœÉœÑŒµ œÉŒÆŒºŒµœÅŒ±;\",  # Greek greeting\n",
        "    \"const handleSubmit = (e) => { e.preventDefault(); setState(newValue); };\",  # React code\n",
        "    \"ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ\",  # Arabic (Hello World)\n",
        "    \"SELECT COUNT(*) OVER (PARTITION BY department) FROM employees;\",  # Advanced SQL\n",
        "    \"pip install tensorflow==2.8.0 torch==2.0.0 transformers==4.28.0\",  # Package installation\n",
        "    \"ÌïúÍ∏ÄÏùÄ ÏÑ∏ÏÉÅÏóêÏÑú Í∞ÄÏû• Í≥ºÌïôÏ†ÅÏù∏ Í∏ÄÏûêÏûÖÎãàÎã§.\",  # Korean (Hangul is the most scientific writing system)\n",
        "    \"{ \\\"name\\\": \\\"John\\\", \\\"age\\\": 30, \\\"city\\\": \\\"New York\\\" }\",  # JSON data\n",
        "    \"CRITICAL: Memory usage exceeded 90% threshold at 02:45:30 UTC\",  # System log\n",
        "    \"@media (max-width: 768px) { .container { flex-direction: column; } }\",  # CSS media query\n",
        "    \"Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144...\",  # Mathematical sequence\n",
        "    \"$ curl -X POST https://api.example.com/v1/data -H \\\"Content-Type: application/json\\\"\",  # CURL command\n",
        "    \"WARNING: Certificate expires in 7 days. Please renew SSL certificate.\",  # Security warning\n",
        "    \"sudo apt-get update && sudo apt-get upgrade -y\",  # Linux command\n",
        "    \"print(f\\\"Current temperature: {temp:.2f}¬∞C at {time:%H:%M:%S}\\\")\",  # Python f-string\n",
        "    \"–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö: —Å–æ–∑–¥–∞–Ω 1000-–∫—É–±–∏—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä\",  # Russian tech news\n",
        "    \"interface User { id: string; name: string; age: number; }\",  # TypeScript interface\n",
        "    \"O Romeo, Romeo! wherefore art thou Romeo?\",  # Shakespeare quote\n",
        "    \"Exception in thread \\\"main\\\" java.lang.NullPointerException at Main.java:42\",  # Java error\n",
        "    \"‰ªäÊó•„ÅØÂØåÂ£´Â±±„Å´Áôª„Çä„Åæ„Åó„Åü„ÄÇÈ†Ç‰∏ä„Åã„Çâ„ÅÆÊôØËâ≤„ÅØÁ¥†Êô¥„Çâ„Åó„Åã„Å£„Åü„Åß„Åô„ÄÇ\"  # Japanese (Climbing Mt. Fuji)\n",
        "]\n",
        "\n",
        "# Define configuration for per-token analysis\n",
        "config_per_token = {\n",
        "    \"mechanism\": \"per_token\",          # Options: \"per_token\", \"per_string\"\n",
        "    \"compute_entropy\": True,\n",
        "    \"compute_varentropy\": True,\n",
        "    \"output_format\": \"dict\"            # Options: \"dict\", \"tensor\", \"list\"\n",
        "}\n",
        "\n",
        "# Define configuration for per-string analysis\n",
        "config_per_string = {\n",
        "    \"mechanism\": \"per_string\",\n",
        "    \"compute_entropy\": True,\n",
        "    \"compute_varentropy\": True,\n",
        "    \"output_format\": \"dict\"\n",
        "}\n",
        "\n",
        "# Compute characteristics per token\n",
        "results_per_token = entropix_model.entropy_characterize(\n",
        "    input_strings=input_strings,\n",
        "    config=config_per_token,\n",
        "    max_length=512  # Adjust as needed\n",
        ")\n",
        "\n",
        "# Compute characteristics per string\n",
        "results_full = entropix_model.entropy_characterize(\n",
        "    input_strings=input_strings,\n",
        "    config=config_per_string,\n",
        "    max_length=512  # Adjust as needed\n",
        ")\n",
        "\n",
        "# Visualize results per token\n",
        "fig_per_token = entropix_model.visualize_results(\n",
        "    results=results_per_token,\n",
        "    config=config_per_token,\n",
        "    title=\"Entropy Analysis (Per Token)\"\n",
        ")\n",
        "fig_per_token.show()\n",
        "\n",
        "# Visualize results per string\n",
        "fig_full = entropix_model.visualize_results(\n",
        "    results=results_full,\n",
        "    config=config_per_string,\n",
        "    title=\"Entropy Analysis (Full String)\"\n",
        ")\n",
        "fig_full.show()\n",
        "\n",
        "# Optional: Save the figures as HTML files\n",
        "# fig_per_token.write_html(\"entropy_analysis_per_token.html\")\n",
        "# fig_full.write_html(\"entropy_analysis_full.html\")\n",
        "\n",
        "# Example of permuting the dataset based on average entropy per token\n",
        "permuted_dataset, sorted_characteristics = entropix_model.permute_dataset(\n",
        "    dataset=input_strings,\n",
        "    config=config_per_token,\n",
        "    sort_by=\"entropy_token_avg\",      # Define your sort key\n",
        "    descending=True,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "print(\"\\nPermuted Dataset Sorted by Average Entropy per Token:\")\n",
        "for idx, (string, entropy) in enumerate(zip(permuted_dataset, sorted_characteristics)):\n",
        "    print(f\"{idx + 1}: {entropy:.4f} - {string}\")\n",
        "\n",
        "# Optionally, visualize the sorted characteristics\n",
        "sorted_results = entropix_model.entropy_characterize(\n",
        "    input_strings=permuted_dataset,\n",
        "    config=config_per_token,\n",
        "    max_length=512\n",
        ")\n",
        "sorted_fig = entropix_model.visualize_results(\n",
        "    results=sorted_results,\n",
        "    config=config_per_token,\n",
        "    title=\"Sorted Entropy Analysis (Per Token)\"\n",
        ")\n",
        "sorted_fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPXQwDvmxtBb"
      },
      "outputs": [],
      "source": [
        "fig_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYyQLav56KZA"
      },
      "outputs": [],
      "source": [
        "fig_per_token"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GkZ6aNBp-6Gb"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
