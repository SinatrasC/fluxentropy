{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GkZ6aNBp-6Gb"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fluxentropy\n",
        "By [Green](https://x.com/myainotez) and [Blue](https://x.com/tensorqt) knights."
      ],
      "metadata": {
        "id": "SyZTa6Ie93QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize"
      ],
      "metadata": {
        "id": "GkZ6aNBp-6Gb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hahhj8LPlhgG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "if device == torch.device(\"cuda\"):\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.cuda.empty_cache()\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "\n",
        "class EntropixModel:\n",
        "    def __init__(self, model, tokenizer, seed: int = 1337, dtype: torch.dtype = torch.bfloat16):\n",
        "        self.model = model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def entropy_characterize(self, input_strings: list, max_length: int = 512):\n",
        "        \"\"\"\n",
        "        Computes the entropy and varentropy per token for a batch of input strings.\n",
        "\n",
        "        Args:\n",
        "            input_strings (list): List of input strings to analyze.\n",
        "            max_length (int): Maximum sequence length for tokenization.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing entropy and varentropy tensors.\n",
        "        \"\"\"\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        # Tokenize input strings with padding and truncation\n",
        "        encodings = self.tokenizer(\n",
        "            input_strings,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "\n",
        "        input_ids = encodings[\"input_ids\"].to(device)\n",
        "        attention_mask = encodings[\"attention_mask\"].to(device)\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # Shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "            # Compute entropy and varentropy per token\n",
        "            entropy_list = []\n",
        "            varentropy_list = []\n",
        "\n",
        "            for i in range(seq_len):\n",
        "                logits_i = logits[:, i, :]  # Logits for position i\n",
        "                log_probs = torch.log_softmax(logits_i, dim=-1)\n",
        "                probs = torch.exp(log_probs)\n",
        "\n",
        "                # Compute entropy\n",
        "                entropy = -torch.sum(probs * log_probs, dim=-1)  # Shape: (batch_size,)\n",
        "\n",
        "                # Compute varentropy\n",
        "                varentropy = torch.sum(probs * (log_probs + entropy.unsqueeze(-1)) ** 2, dim=-1)\n",
        "\n",
        "                entropy_list.append(entropy)\n",
        "                varentropy_list.append(varentropy)\n",
        "\n",
        "            # Stack entropy and varentropy for all positions\n",
        "            entropy_tensor = torch.stack(entropy_list, dim=1)  # Shape: (batch_size, seq_len)\n",
        "            varentropy_tensor = torch.stack(varentropy_list, dim=1)  # Shape: (batch_size, seq_len)\n",
        "\n",
        "            # Mask padding positions\n",
        "            entropy_tensor = entropy_tensor * attention_mask\n",
        "            varentropy_tensor = varentropy_tensor * attention_mask\n",
        "\n",
        "        # Prepare results\n",
        "        results = {\n",
        "            \"entropy\": entropy_tensor.cpu(),\n",
        "            \"varentropy\": varentropy_tensor.cpu(),\n",
        "            \"input_strings\": input_strings,\n",
        "            \"tokens\": [self.tokenizer.convert_ids_to_tokens(ids) for ids in input_ids],\n",
        "            \"attention_mask\": attention_mask.cpu()\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def entropy_characterize_full(self, input_strings: list, max_length: int = 512):\n",
        "        \"\"\"\n",
        "        Computes the entropy and varentropy for the entire input strings at once,\n",
        "        based on the next token prediction after the full input.\n",
        "\n",
        "        Args:\n",
        "            input_strings (list): List of input strings to analyze.\n",
        "            max_length (int): Maximum sequence length for tokenization.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing entropy and varentropy for each input string.\n",
        "        \"\"\"\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        # Tokenize input strings without adding special tokens\n",
        "        encodings = self.tokenizer(\n",
        "            input_strings,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "\n",
        "        input_ids = encodings[\"input_ids\"].to(device)\n",
        "        attention_mask = encodings[\"attention_mask\"].to(device)\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get logits for the next token after the full input\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits  # Shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "            # Get the logits for the next token prediction\n",
        "            next_token_logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
        "\n",
        "            # Compute log probabilities\n",
        "            log_probs = torch.log_softmax(next_token_logits, dim=-1)\n",
        "            probs = torch.exp(log_probs)\n",
        "\n",
        "            # Compute entropy\n",
        "            entropy = -torch.sum(probs * log_probs, dim=-1)  # Shape: (batch_size,)\n",
        "\n",
        "            # Compute varentropy\n",
        "            varentropy = torch.sum(probs * (log_probs + entropy.unsqueeze(-1)) ** 2, dim=-1)\n",
        "\n",
        "        # Prepare results\n",
        "        results = {\n",
        "            \"entropy\": entropy.cpu(),\n",
        "            \"varentropy\": varentropy.cpu(),\n",
        "            \"input_strings\": input_strings,\n",
        "            \"tokens\": [self.tokenizer.convert_ids_to_tokens(ids) for ids in input_ids],\n",
        "            \"attention_mask\": attention_mask.cpu()\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_results(self, results, title=None, height=800):\n",
        "        \"\"\"\n",
        "        Creates interactive visualizations for entropy and varentropy results using plotly.\n",
        "\n",
        "        Args:\n",
        "            results (dict): Dictionary containing entropy_characterize results\n",
        "            title (str, optional): Title for the visualization\n",
        "            height (int, optional): Height of the plot in pixels\n",
        "\n",
        "        Returns:\n",
        "            plotly.graph_objects.Figure: Interactive figure with entropy and varentropy visualizations\n",
        "        \"\"\"\n",
        "        # Determine if results are per-token or full-string\n",
        "        is_full_string = len(results['entropy'].shape) == 1\n",
        "\n",
        "        if is_full_string:\n",
        "            # Visualization for full-string entropy\n",
        "            fig = go.Figure()\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=results['input_strings'],\n",
        "                    y=results['entropy'],\n",
        "                    text=results['entropy'],\n",
        "                    textposition='auto',\n",
        "                    name='Entropy'\n",
        "                )\n",
        "            )\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=results['input_strings'],\n",
        "                    y=results['varentropy'],\n",
        "                    text=results['varentropy'],\n",
        "                    textposition='auto',\n",
        "                    name='Varentropy'\n",
        "                )\n",
        "            )\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=title or 'Entropy and Varentropy Analysis (Full String)',\n",
        "                xaxis_title='Input Strings',\n",
        "                yaxis_title='Value',\n",
        "                barmode='group',\n",
        "                height=height\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # Visualization for per-token entropy\n",
        "            fig = make_subplots(\n",
        "                rows=3, cols=1,\n",
        "                subplot_titles=('Entropy Over Time', 'Varentropy Over Time', 'Token-wise Analysis'),\n",
        "                vertical_spacing=0.1,\n",
        "                row_heights=[0.35, 0.35, 0.3]\n",
        "            )\n",
        "\n",
        "            for batch_idx, input_string in enumerate(results['input_strings']):\n",
        "                # Get masked values using attention mask\n",
        "                mask = results['attention_mask'][batch_idx]\n",
        "                seq_len = mask.sum().item()\n",
        "\n",
        "                # Get tokens for this sequence\n",
        "                tokens = results['tokens'][batch_idx][:seq_len]\n",
        "\n",
        "                # Extract entropy and varentropy values\n",
        "                entropy_values = results['entropy'][batch_idx][:seq_len].numpy()\n",
        "                varentropy_values = results['varentropy'][batch_idx][:seq_len].numpy()\n",
        "\n",
        "                # Create position indices\n",
        "                positions = np.arange(seq_len)\n",
        "\n",
        "                # Add entropy trace\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=positions,\n",
        "                        y=entropy_values,\n",
        "                        mode='lines+markers',\n",
        "                        name=f'Entropy (String {batch_idx + 1})',\n",
        "                        hovertemplate='Position: %{x}<br>Entropy: %{y:.3f}<extra></extra>'\n",
        "                    ),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "                # Add varentropy trace\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=positions,\n",
        "                        y=varentropy_values,\n",
        "                        mode='lines+markers',\n",
        "                        name=f'Varentropy (String {batch_idx + 1})',\n",
        "                        hovertemplate='Position: %{x}<br>Varentropy: %{y:.3f}<extra></extra>'\n",
        "                    ),\n",
        "                    row=2, col=1\n",
        "                )\n",
        "\n",
        "                # Add token-wise heatmap\n",
        "                fig.add_trace(\n",
        "                    go.Heatmap(\n",
        "                        z=[entropy_values, varentropy_values],\n",
        "                        x=tokens,\n",
        "                        y=['Entropy', 'Varentropy'],\n",
        "                        colorscale='Viridis',\n",
        "                        showscale=True,\n",
        "                        hoverongaps=False,\n",
        "                        hovertemplate='Token: %{x}<br>Metric: %{y}<br>Value: %{z:.3f}<extra></extra>'\n",
        "                    ),\n",
        "                    row=3, col=1\n",
        "                )\n",
        "\n",
        "            # Update layout\n",
        "            fig.update_layout(\n",
        "                height=height,\n",
        "                showlegend=True,\n",
        "                title=title or 'Entropy and Varentropy Analysis',\n",
        "                hovermode='closest'\n",
        "            )\n",
        "\n",
        "            # Update axes labels\n",
        "            fig.update_xaxes(title_text='Position', row=1, col=1)\n",
        "            fig.update_xaxes(title_text='Position', row=2, col=1)\n",
        "            fig.update_xaxes(title_text='Tokens', row=3, col=1)\n",
        "\n",
        "            fig.update_yaxes(title_text='Entropy', row=1, col=1)\n",
        "            fig.update_yaxes(title_text='Varentropy', row=2, col=1)\n",
        "\n",
        "        return fig\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    seed = 1337\n",
        "    torch.manual_seed(seed=seed)\n",
        "\n",
        "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    entropix_model = EntropixModel(model, tokenizer, seed=seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "wCO90pDG-1DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Example inputs\n",
        "    input_strings = [\n",
        "        \"The quick brown fox jumps over the lazy dog.\",  # Classic pangram\n",
        "        \"In quantum mechanics, particles can exist in multiple states simultaneously.\",  # Scientific\n",
        "        \"Â•πÁ´ôÂú®Á™óÂâçÔºåÊúõÁùÄËøúÊñπÁöÑÂ±±Â≥∞„ÄÇ\",  # Chinese (Looking at distant mountains)\n",
        "        \"To be, or not to be, that is the question.\",  # Literary/Shakespeare\n",
        "        \"The cryptocurrency market experienced significant volatility today.\",  # Financial news\n",
        "        \"Je pense, donc je suis.\",  # French philosophy (Descartes)\n",
        "        \"üåü Dancing under the moonlight, spirits high and hearts light. üåô\",  # Emojis and poetic\n",
        "        \"SELECT * FROM users WHERE age > 18;\",  # SQL code\n",
        "        \"The neural network achieved 98.5% accuracy on the test dataset.\",  # AI/ML\n",
        "        \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",  # Latin placeholder\n",
        "        \"Breaking: Major breakthrough in fusion energy announced today!\",  # News headline\n",
        "        \"def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)\",  # Python code\n",
        "        \"Step 1: Preheat oven to 350¬∞F. Step 2: Mix ingredients thoroughly.\",  # Recipe instructions\n",
        "        \"Once upon a time, in a galaxy far, far away...\",  # Story opening\n",
        "        \"Error 404: Page not found. Please check the URL and try again.\",  # Technical error\n",
        "        \"Climate change threatens biodiversity in coral reef ecosystems.\",  # Environmental\n",
        "        \"„Åä„ÅØ„Çà„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÅ‰ªäÊó•„ÅØ„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠„ÄÇ\",  # Japanese (Good morning, nice weather today)\n",
        "        \"1234567890 !@#$%^&*()_+ <>?:\\\"{}|\",  # Numbers and special characters\n",
        "        \"URGENT: Meeting rescheduled to 3PM EST - All hands required\",  # Business communication\n",
        "        \"The composition of Bach's fugues demonstrates mathematical precision.\",  # Music analysis\n",
        "        \"Das Leben ist wie ein Fahrrad. Man muss sich vorw√§rts bewegen.\",  # German (Einstein quote)\n",
        "        \"for i in range(len(array)): if array[i] > max_val: max_val = array[i]\",  # More Python code\n",
        "        \"CREATE TABLE employees (id INT PRIMARY KEY, name VARCHAR(255));\",  # SQL DDL\n",
        "        \"La vita √® bella quando si vive con passione.\",  # Italian (Life is beautiful...)\n",
        "        \"RT @SpaceX: Successful launch of Starship prototype #42! üöÄ\",  # Social media\n",
        "        \"–í —Ç–∏—Ö–æ–º –æ–º—É—Ç–µ —á–µ—Ä—Ç–∏ –≤–æ–¥—è—Ç—Å—è.\",  # Russian proverb\n",
        "        \"async function fetchData() { const response = await fetch(url); }\",  # JavaScript async\n",
        "        \"üéÆ Level Up! You've earned 1000 XP and unlocked new achievements! üèÜ\",  # Gaming with emojis\n",
        "        \"<!DOCTYPE html><html><head><title>Hello World</title></head></html>\",  # HTML\n",
        "        \"Hola mundo, ¬øc√≥mo est√°s hoy?\",  # Spanish greeting\n",
        "        \"import numpy as np; X = np.array([[1, 2], [3, 4]])\",  # Scientific Python\n",
        "        \"Breaking News: Artificial Intelligence Achieves New Milestone in Protein Folding\",  # Science news\n",
        "        \"public class HelloWorld { public static void main(String[] args) {} }\",  # Java\n",
        "        \"The mitochondria is the powerhouse of the cell.\",  # Biology\n",
        "        \"git commit -m \\\"Fix: resolve memory leak in main loop\\\"\",  # Git command\n",
        "        \"‡§Ö‡§§‡§ø‡§•‡§ø ‡§¶‡•á‡§µ‡•ã ‡§≠‡§µ:\",  # Sanskrit (Guest is God)\n",
        "        \"try { throw new Error('Test'); } catch (e) { console.log(e); }\",  # JavaScript error handling\n",
        "        \"Dans les champs de l'observation, le hasard ne favorise que les esprits pr√©par√©s.\",  # French (Pasteur)\n",
        "        \"docker run -d -p 80:80 nginx:latest\",  # Docker command\n",
        "        \"While(true) { System.out.println(\\\"Hello, World!\\\"); }\",  # Infinite loop\n",
        "        \"kubectl get pods -n kubernetes-dashboard\",  # Kubernetes command\n",
        "        \"ŒßŒ±ŒØœÅŒµœÑŒµ! Œ†œéœÇ ŒµŒØœÉœÑŒµ œÉŒÆŒºŒµœÅŒ±;\",  # Greek greeting\n",
        "        \"const handleSubmit = (e) => { e.preventDefault(); setState(newValue); };\",  # React code\n",
        "        \"ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ\",  # Arabic (Hello World)\n",
        "        \"SELECT COUNT(*) OVER (PARTITION BY department) FROM employees;\",  # Advanced SQL\n",
        "        \"pip install tensorflow==2.8.0 torch==2.0.0 transformers==4.28.0\",  # Package installation\n",
        "        \"ÌïúÍ∏ÄÏùÄ ÏÑ∏ÏÉÅÏóêÏÑú Í∞ÄÏû• Í≥ºÌïôÏ†ÅÏù∏ Í∏ÄÏûêÏûÖÎãàÎã§.\",  # Korean (Hangul is the most scientific writing system)\n",
        "        \"{ \\\"name\\\": \\\"John\\\", \\\"age\\\": 30, \\\"city\\\": \\\"New York\\\" }\",  # JSON data\n",
        "        \"CRITICAL: Memory usage exceeded 90% threshold at 02:45:30 UTC\",  # System log\n",
        "        \"@media (max-width: 768px) { .container { flex-direction: column; } }\",  # CSS media query\n",
        "        \"Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144...\",  # Mathematical sequence\n",
        "        \"$ curl -X POST https://api.example.com/v1/data -H \\\"Content-Type: application/json\\\"\",  # CURL command\n",
        "        \"WARNING: Certificate expires in 7 days. Please renew SSL certificate.\",  # Security warning\n",
        "        \"sudo apt-get update && sudo apt-get upgrade -y\",  # Linux command\n",
        "        \"print(f\\\"Current temperature: {temp:.2f}¬∞C at {time:%H:%M:%S}\\\")\",  # Python f-string\n",
        "        \"–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö: —Å–æ–∑–¥–∞–Ω 1000-–∫—É–±–∏—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä\",  # Russian tech news\n",
        "        \"interface User { id: string; name: string; age: number; }\",  # TypeScript interface\n",
        "        \"O Romeo, Romeo! wherefore art thou Romeo?\",  # Shakespeare quote\n",
        "        \"Exception in thread \\\"main\\\" java.lang.NullPointerException at Main.java:42\",  # Java error\n",
        "        \"‰ªäÊó•„ÅØÂØåÂ£´Â±±„Å´Áôª„Çä„Åæ„Åó„Åü„ÄÇÈ†Ç‰∏ä„Åã„Çâ„ÅÆÊôØËâ≤„ÅØÁ¥†Êô¥„Çâ„Åó„Åã„Å£„Åü„Åß„Åô„ÄÇ\"  # Japanese (Climbing Mt. Fuji)\n",
        "    ]\n",
        "\n",
        "    results_per_token = entropix_model.entropy_characterize(input_strings)\n",
        "    results_full = entropix_model.entropy_characterize_full(input_strings)\n",
        "\n",
        "    fig_per_token = entropix_model.visualize_results(results_per_token, title=\"Entropy Analysis (Per Token)\")\n",
        "    fig_full = entropix_model.visualize_results(results_full, title=\"Entropy Analysis (Full String)\")\n",
        "\n",
        "    # Save or display the figures\n",
        "    # fig_per_token.write_html(\"entropy_analysis_per_token.html\")\n",
        "    # fig_full.write_html(\"entropy_analysis_full.html\")\n"
      ],
      "metadata": {
        "id": "W2J8C6hd-lkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_full"
      ],
      "metadata": {
        "id": "qPXQwDvmxtBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_per_token"
      ],
      "metadata": {
        "id": "BYyQLav56KZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}